{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/grad/jmurdoch/.local/lib/python3.5/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../visualization')\n",
    "sys.path.append('../acd/util')\n",
    "sys.path.append('../acd/scores')\n",
    "sys.path.append('../acd/agglomeration')\n",
    "import viz_1d as viz\n",
    "import tiling_1d as tiling\n",
    "import agg_1d as agg\n",
    "import cd\n",
    "import score_funcs\n",
    "\n",
    "\n",
    "# form class to hold data\n",
    "class B:\n",
    "    def __init__(self):\n",
    "        self.text = Variable(torch.zeros(1).cuda())\n",
    "\n",
    "sys.path.append('../dsets/sst')\n",
    "from dsets.sst import dset\n",
    "sst_pkl = pickle.load(open('../dsets/sst/sst.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../dsets/sst/sst.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/linux/anaconda3/lib/python3.5/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'model.LSTMSentiment' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/linux/anaconda3/lib/python3.5/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/linux/anaconda3/lib/python3.5/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/linux/anaconda3/lib/python3.5/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded onto gpu...\n"
     ]
    }
   ],
   "source": [
    "snapshot_file = '../dsets/sst/sst.model'\n",
    "model = dset.get_model(snapshot_file).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dsets/sst/model.py:29: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  lstm_out, self.hidden = self.lstm(vecs, self.hidden)\n",
      "../acd/agglomeration/agg_1d.py:31: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  mask = scores >= thresh\n",
      "/accounts/grad/jmurdoch/.local/lib/python3.5/site-packages/numpy/lib/function_base.py:4033: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e116e2684db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text_orig' is not defined"
     ]
    }
   ],
   "source": [
    "# base parameters\n",
    "sweep_dim = 1 # how large chunks of text should be considered (1 for words)\n",
    "method = 'cd' # build_up, break_down, cd\n",
    "percentile_include = 99.5 # keep this very high so we don't add too many words at once\n",
    "num_iters = 25 # maximum number of iterations (rarely reached)\n",
    "\n",
    "# text and label\n",
    "sentence = ['a', 'great', 'ensemble', 'cast', 'ca', 'n\\'t', 'lift', 'this', 'heartfelt', 'enterprise', 'out', 'of', 'the', 'familiar', '.'] # note this is a real example from the dataset\n",
    "# sentence = ['not', 'good', ',', 'not', 'bad', ',', 'just', 'okay'] # any text with words from this dataset can be interpreted\n",
    "label = 1 # 0 if positive 1 if negative\n",
    "\n",
    "\n",
    "def batch_from_str_list(s):\n",
    "    batch = B()\n",
    "    nums = np.expand_dims(np.array([sst_pkl['stoi'][x] for x in s]).transpose(), axis=1)\n",
    "    batch.text.data = torch.LongTensor(nums).cuda()\n",
    "    return batch\n",
    "\n",
    "# prepare inputs\n",
    "batch = batch_from_str_list(sentence)\n",
    "scores_all = model(batch).data.cpu().numpy()[0] # predict\n",
    "label_pred = np.argmax(scores_all) # get predicted class\n",
    "\n",
    "# agglomerate\n",
    "lists = agg.agglomerate(model, batch, percentile_include, method, sweep_dim, # only works for sweep_dim = 1\n",
    "                    sentence, label_pred, num_iters=num_iters) # see agg_1d.agglomerate to understand what this dictionary contains\n",
    "lists = agg.collapse_tree(lists) # don't show redundant joins\n",
    "\n",
    "# visualize\n",
    "viz.word_heatmap(text_orig, lists, label_pred, label, fontsize=9)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
